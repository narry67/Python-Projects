Data Description: The Metropolitan Museum of Art presents over 5,000 years of art from around the world for everyone to experience and enjoy. The Museum lives in three iconic sites in New York City—The Met Fifth Avenue, The Met Breuer, and The Met Cloisters. Millions of people also take part in The Met experience online. 
Since it was founded in 1870, The Met has always aspired to be more than a treasury of rare and beautiful objects. Every day, art comes alive in the Museum's galleries and through its exhibitions and events, revealing both new ideas and unexpected connections across time and across cultures. 
The Metropolitan Museum of Art provides select datasets of information on more than 470,000 artworks in its Collection for unrestricted commercial and noncommercial use. 

Critical Details and Instructions:
i. Included with these directions should be a .csv file (MetObjects_Subset.csv) that consists of only a small subset of objects (~17.3k) in the museum.  You should use this file as a basis for all instructions that follow.  
ii. You must use either a .ipynb notebook with separate cells per problem or a .py file with separate functions per problem in your submission.  
iii. For problems 1-5, you can manipulate the data-frames/dictionaries as you see fit and using whatever functions/libraries you want.  However, it is critically important that your end results for each problem match the provided variable name (ex: the result of problem 1 is called df_init) so that they are accessible for grading.
iv. With the exception of problem 1 (which is trivial) you should include a few comments in your code that make it clear what your thought process and/or code does to address each problem.
1. Load the .csv file into a pandas data-frame (DF) called df_init with appropriate rows and columns.
2. Many columns of this data are missing entirely (i.e. no entries are present) or have a majority of missing values for each object entry.  Use Python to determine which columns are missing for at least 50% of the provided objects and create a modified version of the data-frame from problem 1 without these columns called df_prob2.
Hint 1: remember that drop is a simple way to remove a column or columns.
Hint 2: You may want to look into the pandas member function isna. 
3. Find the 10 most common values for “Object Name” for the data-frame from #3.  Filter out any rows with objects not among these 10 most common ones and store the result (a data-frame with common objects only) into a data-frame named df_prob3.
4. Most objects are associated with a country.  Compute the percentage of objects from df_prob3 whose “Country” column value matches “United States”, “Mexico”, “Canada”, and “Other” (anything else, including NaN values).  Store these results in a dictionary named dict_prob4, where each key is an entry (“United States”, “Mexico”, “Canada”, or “Other”) and the paired value is the corresponding percentage.  Your percentages should add up to 100.
5. Most of the objects in the dataset include a completion date (“Object End Date”).  For this problem, you should create a dictionary dict_prob5 with ten keys corresponding to each of the decades in the 20th century (1900s, 1910s,…,1990s), with value pairs corresponding to the count (not percentage) of objects from df_prob3 completed in the corresponding decade.  (For example, if 5 objects were completed in the 1990s, the dictionary would include {…‘1990s’: 5}).
6. Provide plots (histograms/bar plots or line plots) for each of the dictionaries in problems 




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df_init = pd.DataFrame()
df_prob2 = pd.DataFrame()
df_prob3 = pd.DataFrame()
columns = []
tencommonvalues = []
dict_prob4 = {}
dict_prob5 = {}
#pd.set_option('display.max_columns', None)
def loadcsv(x):
    global df_init
    df_init = pd.read_csv(x)

def problem2():
    global columns
    global rows
    global df_prob2
                                                                  #use shape function to get number of rows of data
    rows = df_init.shape[0]                                     #columns
    columns = df_init.columns
    df_prob2 = df_init.copy(deep=True)                            # dont want to change og data. might not be needed.
    for x in columns:
        if df_prob2[x].isna().sum() >= rows/2:                    #if missing value of column > half then drop
            df_prob2.drop(x,inplace=True, axis=1)

def problem3():                                                    # use valuecounts function and index to get most common object names
    global df_prob2                                                 # use isin function to get booleans to only pull common value rows
    global df_prob3
    global tencommonvalues
    tencommonvalues = df_prob2["Object Name"].value_counts().index.tolist()[:10]
    df_prob3 = df_prob2[df_prob2["Object Name"].isin(tencommonvalues)]

def problem4():
    global dict_prob4                                                 #navalues = count values not in specified list. fakedf = fill na to identify missing values for valuecountfunction
                                                                       # cvalues = countries. values = quantity. pvalues = percent.
                                                                       # use nparray to divide entire list by sum of all occurances to get percent
    navalues = 0
    fakedf = df_prob3.fillna("NA")
    cvalues = fakedf["Country"].value_counts().index.tolist()
    values = fakedf["Country"].value_counts().values.tolist()
    pvalues = np.array(fakedf["Country"].value_counts().values.tolist())/sum(values)*100
    for v in range(len(cvalues)):                                                         # store UMC values in dictionary
                                                                                          # add up total non UMC values
        if cvalues[v] in ["United States", "Mexico", "Canada"]:
            dict_prob4[cvalues[v]] = pvalues[v]
        else:
            navalues += values[v]
    navalues = navalues/sum(values)*100
    dict_prob4["Canada"] = 0                                                            #include canada since it waasnt found
    dict_prob4["NA"] = navalues

def problem5():
    global dict_prob5
    count = 0
    for x in range(10):                           # quickly create keys for new dictionary to have all decades of 1900s
        dict_prob5[1900 + count] = 0
        count += 10
    count = 0
    fakedf = df_prob3
    cvalues = fakedf["Object End Date"].value_counts().index.tolist()  #country values
    values = fakedf["Object End Date"].value_counts().values.tolist()  # frequency values of year
    for y in range(10):    # ten decades
        for x in range(len(cvalues)):  #run through every value for each decade and add to dictionary if on appropriate decade
            if cvalues[x] >= 1900 + count and cvalues[x] < 1910 + count:
                dict_prob5[1900 + count] += values[x]
        count += 10
    dict_prob5 = {f'{x}s':v for x,v in dict_prob5.items()}     #add s for formatting
def problem6():      #barcharts
    plt.bar(dict_prob4.keys(),dict_prob4.values())
    plt.title("Dict 4")
    plt.ylabel("Percent")
    plt.show()
    plt.clf()          #clear data so can make second chart
    plt.bar(dict_prob5.keys(),dict_prob5.values(), width = .8)
    plt.title('Dict 5')
    plt.ylabel("Qty")
    plt.show()
                                                            #note: only problem one takes an argument aka the file path
loadcsv("C:\\Users\\gohar\\Downloads\\testdata.csv")
problem2()
problem3()
problem4()
problem5()
problem6()
